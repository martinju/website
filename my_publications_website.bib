@mastersthesis{jullum2012focused,
  title={Focused Information criteria for selecting among parametric and nonparametric models},
  author={Jullum, Martin},
  year={2012},
  url={http://urn.nb.no/URN:NBN:no-32391},
  school={Department of Mathematics, University of Oslo},
  abstract = {In this thesis we develop Focus Information Criteria (FIC) for a number of situations concerning model selection among nonparametric and parametric models. We mainly handle the one-sample case of fully observed independent identically distributed (i.i.d.) data, but also work in the more general setting with censored data. In addition, the regression setting and two-sample situation are discussed. Our criteria are based on asymptotic theory. Maximum likelihood theory outside model conditions and nonparametrics via statistical functionals are central.
For our main criterion we state weak sufficient conditions, show strong consistency of the intermediate estimators and study the criterion’s properties both in the limit and for finite samples. It turns out that the criterion has certain desirable properties making it asymptotically superior to other information criteria like AIC and BIC in a fairly wide class of situations.
We also develop weighted versions of some of the schemes and discuss the link between such a criterion and a certain class of goodness of fit tests. Model averaging is also discussed in terms of the derived schemes. We propose a model averaging generalization of the FIC schemes and derive the limiting distribution of the final estimator under a few conditions.
Some of the criteria are applied to real data examples using the R function that is developed.}
}


@inproceedings{jullum2015approximate,
  title={An approximate Bayesian inversion framework based on local-Gaussian likelihoods},
  author={Jullum, Martin and Kolbj{\o}rnsen, Odd},
  booktitle={Petroleum Geostatistics 2015},
  pages={cp--456},
  year={2015},
  organization={European Association of Geoscientists \& Engineers},
  doi={https://doi.org/10.3997/2214-4609.201413634},
  abstract={We derive a Bayesian statistical procedure for inversion of geophysical data to rock properties. The procedure is for simplicity presented in the seismic AVO setting where rock properties influence the data through elastic parameters. The framework may however easily be extended. The procedure combines sampling based techniques and a compound Gaussian approximation to assess local approximations to marginal posterior distributions of rock properties, which the inversion is based on. The framework offers a range of approximations where inversion speed and accuracy may be balanced. The approach is also well suited for parallelisation, making it attractive for large inversion problems. We apply the procedure to a 4D CO2 monitoring case with focus on predicting saturation content. Promising results are obtained for both synthetic and real data. Finally we compare our method with regular linear Gaussian inversion for density prediction, where our method gives an improved fit.}
}

@inproceedings{hermansen2015parametric,
  title={Parametric or nonparametric: The FIC approach for stationary time series},
  author={Hermansen, Gudmund Horn and Hjort, Nils Lid and Jullum, Martin},
  booktitle={Proceedings of the 60th World Statistics Congress of the International Statistical Institute},
  pages={4827--4832},
  year={2015},
abstract = {We seek to narrow the gap between parametric and nonparametric modelling of stationary time series pro-
cesses. The approach is inspired by recent advances in focused inference and model selection techniques.Our
paper generalises and extends current work by developing a new version of the focused information criterion
(FIC), directly comparing the performance of both parametric and nonparametric time series models. This is
achieved by comparing the mean squared error for estimating a focus parameter under consideration, for each
candidate model. In particular this yields FIC formulae for covariances or correlations at specified lags, for
the probability of reaching a threshold, etc. Suitable weighted average versions, the AFIC, also lead to model
selection strategies for finding the best model for the purpose of estimating e.g. a sequence of correlations.}
}


@article{jullum2016gaussian,
  title={A Gaussian-based framework for local Bayesian inversion of geophysical data to rock properties},
  author={Jullum, Martin and Kolbj{\o}rnsen, Odd},
  journal={Geophysics},
  volume={81},
  number={3},
  pages={R75--R87},
  year={2016},
  publisher={Society of Exploration Geophysicists},
  doi={https://doi.org/10.1190/geo2015-0314.1},
  abstract={Working in a Bayesian framework, we have derived a procedure for inverting rock properties based on geophysical data. The purpose was to arrive at a widely applicable and general procedure in which few and weak assumptions are required for application to various inverse problems within the geophysical industry. Our Bayesian statistical approach combines sampling-based techniques and Gaussian approximations to assess local approximations to quantities related to the posterior distribution of rock properties. These approximated quantities define the Bayesian inversion. A conceptual advantage of our approach is that there are few restrictions on the initial model, allowing realistic statistical models to be approximated directly. The methodology is easily parallelized and offers a range of procedures, which gives a trade-off between inversion speed and accuracy. We have tested the approach in a monitoring setting using seismic amplitudes by evaluating a synthetic case and real data from the Sleipner CO2 injection project. For the synthetic case, the inversion results correspond well with the rock properties used to generate the data and the posterior distribution derived using an MCMC approach. We also found improved accuracy compared with a frequently used Gaussian inversion approach. In the real data case, we clearly identified high-saturation layers present in previous qualitative interpretations.}
}

@article{kolbjornsen2016bayesian,
  title={Bayesian AVO inversion to rock properties using a local neighborhood in a spatial prior model},
  author={Kolbj{\o}nsen, Odd and Buland, Arild and Hauge, Ragnar and R{\o}e, Per and Jullum, Martin and Metcalfe, Richard William and Skj{\ae}veland, {\O}yvind},
  journal={The Leading Edge},
  volume={35},
  number={5},
  pages={431--436},
  year={2016},
  publisher={Society of Exploration Geophysicists}
}

@phdthesis{jullum2016new,
  title={New focused approaches to topics within model selection and approximate Bayesian inversion},
  author={Jullum, Martin},
  year={2016},
  doi={http://urn.nb.no/URN:NBN:no-53863},
  school={Department of Mathematics, University of Oslo},
  abstract = {This thesis comprises the work conducted during three years as a PhD student at the Department of Mathematics at the University of Oslo (UiO). I will be looking back at these years as an enjoyable phase of my life, in which I learned a lot. The period has also been tough, though -- possibly even tougher than I expected it to be. With the finished thesis in my hand, there is however no doubt it was worth it. Still, I have to agree with the author Joseph Epstein who noted that it is a lot better to have written, than to actually be writing. The final product constituting my PhD thesis is inarguably positively correlated with the original project description, although the rho is far from one. Through these years I have been working with methodology within a broad range of fields across the science of statistics. Among them are approximate Bayesian inference, asymptotic theory, Bayesian statistics, copulae, density estimation, frequentist statistics, functional differentiation, Gaussian distribution theory, geostatistics, inverse problems, Markov chain Monte Carlo (MCMC), model averaging, model selection, spatial statistics, stochastic process theory, survival analysis, time series modelling – and I even had to learn a little bit of geophysics, petrophysics and rock physics. I do by no means claim to master all these subjects, but I have learned a fair amount about all of them, and for that I feel incredibly lucky.

Upon completing this thesis, I am deeply indebted to my two supervisors Nils Lid Hjort and Odd Kolbjørnsen. I am truly grateful for how you inspired me, and the eagerness you showed while working with the various projects. I will sincerely like to thank you both for that. You also supported me and made it possible for me to spend the autumn of 2014 at Stanford University, visiting Paul Switzer at the Department of Statistics. Paul was an outstanding host during some incredible months over there – our delightful academic and non-academic discussions will not be easily forgotten. Being founded by Statistics for Innovation (SFI2), a centre for
research-based innovation, I was lucky enough to be awarded with two offices in Oslo; one at the campus at Blindern and one at the Norwegian Computing Center (NR) at Forskningsparken. Without even being employed at NR, I was very well taken care of and included in the SAND group. I am thankful for that additional dimension and opportunity to learn, and for being exposed to the weekly dose of Thursday-buns – that will be missed! I would also like to thank all my colleagues, both at the statistics group at UiO and the SAND group at NR. Special thanks go to my ‘roommates’ Marie at NR and Reinaldo at UiO for all our inviting discussions and fascinating conversations, and to Gudmund Horn Hermansen for co-authoring one of the papers in the thesis. Finally, I would like to thank my friends, family, and ‘family-in-law’ for filling my life with joy – especially my wonderful Elin for putting up with me, supporting and understanding me, even though I know you really wished I was rather spending those late evenings and weekends with you.}
}


@article{jullum2017parametric,
  title={Parametric or nonparametric: The FIC approach},
  author={Jullum, Martin and Hjort, Nils Lid},
  journal={Statistica Sinica},
  pages={951--981},
  year={2017},
  publisher={Institute of Statistical Science, Academia Sinica and International Chinese Statistical Association},
  doi={https://doi.org/10.5705/ss.202015.0364},
  abstract={Should one rely on a parametric or nonparametric model when analysing a given data set? This classic question cannot be answered by traditional model selection criteria like AIC and BIC, since a nonparametric model has no likelihood. The purpose of the present paper is to develop a focused information criterion (FIC) for comparing general non-nested parametric models with a nonparametric alternative. It relies in part on the notion of a focus parameter, a population quantity of particular interest in the statistical analysis. The FIC compares and ranks candidate models based on estimated precision of the different model-based estimators for the focus parameter. It has earlier been developed for several classes of problems, but mainly involving parametric models. The new FIC, including also nonparametrics, is novel also in the mathematical context, being derived without the local neighbourhood asymptotics underlying previous versions of FIC. Certain average-weighted versions, called AFIC, allowing several focus parameters to be considered simultaneously, are also developed. We concentrate on the standard i.i.d. setting and certain direct extensions thereof, but also sketch further generalisations to other types of data. Theoretical and simulation-based results demonstrate desirable properties and satisfactory performance.}
}

@article{jullum2019price,
  title={What price semiparametric Cox regression?},
  author={Jullum, Martin and Hjort, Nils Lid},
  journal={Lifetime data analysis},
  volume={25},
  number={3},
  pages={406--438},
  year={2019},
  publisher={Springer},
  doi={https://doi.org/10.1007/s10985-018-9450-7},
  abstract={Cox’s proportional hazards regression model is the standard method for modelling censored life-time data with covariates. In its standard form, this method relies on a semiparametric proportional hazards structure, leaving the baseline unspecified. Naturally, specifying a parametric model also for the baseline hazard, leading to fully parametric Cox models, will be more efficient when the parametric model is correct, or close to correct. The aim of this paper is two-fold. (a) We compare parametric and semiparametric models in terms of their asymptotic relative efficiencies when estimating different quantities. We find that for some quantities the gain of restricting the model space is substantial, while it is negligible for others. (b) To deal with such selection in practice we develop certain focused and averaged focused information criteria (FIC and AFIC). These aim at selecting the most appropriate proportional hazards models for given purposes. Our methodology applies also to the simpler case without covariates, when comparing Kaplan–Meier and Nelson–Aalen estimators to parametric counterparts. Applications to real data are also provided, along with analyses of theoretical behavioural aspects of our methods.}
}

@article{sellereite2020shapr,
  title={shapr: An R-package for explaining machine learning models with dependence-aware Shapley values},
  author={Sellereite, Nikolai and Jullum, Martin},
  journal={The Journal of Open Source Software},
  volume={5},
  number={46},
  pages={2027},
  year={2020},
  publisher={The Open Journal},
  doi={https://doi.org/10.21105/joss.02027}
}



@article{jullum2020investigating,
  title={Investigating mesh-based approximation methods for the normalization constant in the log Gaussian Cox process likelihood},
  author={Jullum, Martin},
  journal={Stat},
  volume={9},
  number={1},
  pages={e285},
  year={2020},
  publisher={Wiley Online Library},
  doi={https://doi.org/10.1002/sta4.285},
  abstract={The log Gaussian Cox process (LGCP) is a frequently applied method for modeling point pattern data. The normalization constant of the LGCP likelihood involves an integral over a latent field. That integral is computationally expensive, making it troublesome to perform inference with standard methods. The so-called stochastic partial differential equation–integrated nested Laplace approximation (SPDE-INLA) framework enables fast approximate inference for a range of hierarchical models, where a key component is to approximate the latent field by a triangulated mesh. Recent research has made it possible to fit LGCP models with this framework using an approximate integration method to compute the integral. We carefully describe several alternative variants of that approximate integration method and derive an analytical formula for the integral in question, which actually is exact under the triangular mesh assumption used by SPDE-INLA. We compare the different integration strategies through a comprehensive simulation study and find that the analytical formula is often more accurate, but not always. Among the approximate integration methods, we recommend a simple extension to a method implemented in an R-package for fitting LGCP models.}
}

@article{otneim2020pairwise,
  title={Pairwise local Fisher and Na{\"\i}ve Bayes: Improving two standard discriminants},
  author={Otneim, H{\aa}kon and Jullum, Martin and Tj{\o}stheim, Dag},
  journal={Journal of Econometrics},
  volume={216},
  number={1},
  pages={284--304},
  year={2020},
  publisher={Elsevier},
  doi={https://doi.org/10.1016/j.jeconom.2020.01.019},
  abstract={The Fisher discriminant is probably the best known likelihood discriminant for continuous data. Another benchmark discriminant is the naive Bayes, which is based on marginals only. In this paper we extend both discriminants by modeling dependence between pairs of variables. In the continuous case this is done by local Gaussian versions of the Fisher discriminant. In the discrete case the naive Bayes is extended by taking geometric averages of pairwise joint probabilities. We also indicate how the two approaches can be combined for mixed continuous and discrete data. The new discriminants show promising results in a number of simulation experiments and real data illustrations.}
}


@article{jullum2020detecting,
  title={Detecting money laundering transactions with machine learning},
  author={Jullum, Martin and L{\o}land, Anders and Huseby, Ragnar Bang and {\AA}nonsen, Geir and Lorentzen, Johannes},
  journal={Journal of Money Laundering Control},
  year={2020},
  publisher={Emerald Publishing Limited},
  doi={https://doi.org/10.1108/JMLC-07-2019-0055},
  abstract={
Purpose
The purpose of this paper is to develop, describe and validate a machine learning model for prioritising which financial transactions should be manually investigated for potential money laundering. The model is applied to a large data set from Norway’s largest bank, DNB.

Design/methodology/approach
A supervised machine learning model is trained by using three types of historic data: “normal” legal transactions; those flagged as suspicious by the bank’s internal alert system; and potential money laundering cases reported to the authorities. The model is trained to predict the probability that a new transaction should be reported, using information such as background information about the sender/receiver, their earlier behaviour and their transaction history.

Findings
The paper demonstrates that the common approach of not using non-reported alerts (i.e. transactions that are investigated but not reported) in the training of the model can lead to sub-optimal results. The same applies to the use of normal (un-investigated) transactions. Our developed method outperforms the bank’s current approach in terms of a fair measure of performance.

Originality/value
This research study is one of very few published anti-money laundering (AML) models for suspicious transactions that have been applied to a realistically sized data set. The paper also presents a new performance measure specifically tailored to compare the proposed method to the bank’s existing AML system.
}
}

@article{jullum2020estimating,
  title={Estimating seal pup production in the Greenland Sea by using Bayesian hierarchical modelling},
  author={Jullum, Martin and Thorarinsdottir, Thordis and Bachl, Fabian E},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={69},
  number={2},
  pages={327--352},
  year={2020},
  publisher={Wiley Online Library},
  doi={https://doi.org/10.1111/rssc.12397},
  abstract={The Greenland Sea is an important breeding ground for harp and hooded seals. Estimates of annual seal pup production are critical factors in the estimation of abundance that is needed for management of the species. These estimates are usually based on counts from aerial photographic surveys. However, only a minor part of the whelping region can be photographed, because of its large extent. To estimate total seal pup production, we propose a Bayesian hierarchical modelling approach motivated by viewing the seal pup appearances as a realization of a log‐Gaussian Cox process by using covariate information from satellite imagery as a proxy for ice thickness. For inference, we utilize the stochastic partial differential equation module of the integrated nested Laplace approximation framework. In a case‐study using survey data from 2012, we compare our results with existing methodology in a comprehensive cross‐validation study. The results of the study indicate that our method improves local estimation performance, and that the increased uncertainty of prediction of our method is required to obtain calibrated count predictions. This suggests that the sampling density of the survey design may not be sufficient to obtain reliable estimates of seal pup production.}
}

@inproceedings{redelmeier2020explaining,
  title={Explaining predictive models with mixed features using Shapley values and conditional inference trees},
  author={Redelmeier, Annabelle and Jullum, Martin and Aas, Kjersti},
  booktitle={International Cross-Domain Conference for Machine Learning and Knowledge Extraction},
  editor={Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  pages={117--137},
  year={2020},
  organization={Springer},
  doi={https://doi.org/10.1007/978-3-030-57321-8_7},
  isbn={978-3-030-57321-8},
  abstract={It is becoming increasingly important to explain complex, black-box machine learning models. Although there is an expanding literature on this topic, Shapley values stand out as a sound method to explain predictions from any type of machine learning model. The original development of Shapley values for prediction explanation relied on the assumption that the features being described were independent. This methodology was then extended to explain dependent features with an underlying continuous distribution. In this paper, we propose a method to explain mixed (i.e. continuous, discrete, ordinal, and categorical) dependent features by modeling the dependence structure of the features using conditional inference trees. We demonstrate our proposed method against the current industry standards in various simulation studies and find that our method often outperforms the other approaches. Finally, we apply our method to a real financial data set used in the 2018 FICO Explainable Machine Learning Challenge and show how our explanations compare to the FICO challenge Recognition Award winning team.}
}

@article{aas2021explaining,
  title={Explaining individual predictions when features are dependent: More accurate approximations to Shapley values},
  author={Aas, Kjersti and Jullum, Martin and L{\o}land, Anders},
  journal={Artificial Intelligence},
  volume={298},
  pages={103502},
  year={2021},
  publisher={Elsevier},
  doi={https://doi.org/10.1016/j.artint.2021.103502},
  abstract={Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for prediction. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values.}
}

@article{aas2021explainingb,
  title={Explaining predictive models using Shapley values and non-parametric vine copulas},
  author={Aas, Kjersti and Nagler, Thomas and Jullum, Martin and L{\o}land, Anders},
  journal={Dependence Modeling},
  volume={9},
  number={1},
  pages={62--81},
  year={2021},
  publisher={De Gruyter},
  doi={https://doi.org/10.1515/demo-2021-0103},
  abstract={In this paper the goal is to explain predictions from complex machine learning models. One method that has become very popular during the last few years is Shapley values. The original development of Shapley values for prediction explanation relied on the assumption that the features being described were independent. If the features in reality are dependent this may lead to incorrect explanations. Hence, there have recently been attempts of appropriately modelling/estimating the dependence between the features. Although the previously proposed methods clearly outperform the traditional approach assuming independence, they have their weaknesses. In this paper we propose two new approaches for modelling the dependence between the features. Both approaches are based on vine copulas, which are flexible tools for modelling multivariate
non-Gaussian distributions able to characterise a wide range of complex dependencies. The performance of the proposed methods is evaluated on simulated data sets and a real data set. The experiments demonstrate that the vine copula approaches give more accurate approximations to the true Shapley values than their competitors.}
}

@inproceedings{framling2021comparison,
  title={Comparison of Contextual Importance and Utility with LIME and Shapley Values},
  author={Fr{\"a}mling, Kary and Westberg, Marcus and Jullum, Martin and Madhikermi, Manik and Malhi, Avleen},
  booktitle={International Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Systems},
  pages={39--54},
  year={2021},
  editor={Calvaresi, Davide and Najjar, Amro and Winikoff, Michael and Fr{\"a}mling, Kary},
  organization={Springer},
  doi={https://doi.org/10.1007/978-3-030-82017-6_3},
  abstract={Different explainable AI (XAI) methods are based on different notions of ‘ground truth’. In order to trust explanations of AI systems, the ground truth has to provide fidelity towards the actual behaviour of the AI system. An explanation that has poor fidelity towards the AI system’s actual behaviour can not be trusted no matter how convincing the explanations appear to be for the users. The Contextual Importance and Utility (CIU) method differs from currently popular outcome explanation methods such as Local Interpretable Model-agnostic Explanations (LIME) and Shapley values in several ways. Notably, CIU does not build any intermediate interpretable model like LIME, and it does not make any assumption regarding linearity or additivity of the feature importance. CIU also introduces the value utility notion and a definition of feature importance that is different from LIME and Shapley values. We argue that LIME and Shapley values actually estimate ‘influence’ (rather than ‘importance’), which combines importance and utility. The paper compares the three methods in terms of validity of their ground truth assumption and fidelity towards the underlying model through a series of benchmark tasks. The results confirm that LIME results tend not to be coherent nor stable. CIU and Shapley values give rather similar results when limiting explanations to ‘influence’. However, by separating ‘importance’ and ‘utility’ elements, CIU can provide more expressive and flexible explanations than LIME and Shapley values.}
}


@inproceedings{jullum2021efficient,
  title={Efficient and simple prediction explanations with groupShapley: A practical perspective},
  author={Jullum, Martin and Redelmeier, Annabelle and Aas, Kjersti},
  booktitle={Italian Workshop on Explainable Artificial Intelligence 2021},
  pages={28--43},
  year={2021},
  editor={Cataldo Musto and Riccardo Guidotti and Anna Monreale and Giovanni Semeraro},
  organization={XAI.it},
  doi={http://ceur-ws.org/Vol-3014/paper3.pdf},
  abstract={Shapley values has established itself as one of the most appropriate and theoretically sound frameworks for explaining predictions from complex machine learning models. The popularity of Shapley values in the explanation setting is probably due to Shapley values’ unique theoretical properties. The main drawback with Shapley values, however, is that the computational complexity grows exponentially in the number of input features, making it unfeasible in many real world situations where there could be hundreds or thousands of features. Furthermore, with many (dependent) features, presenting/visualizing and interpreting the computed Shapley values also become challenging. The present paper introduces and showcases a method that we call groupShapley. The idea of the method is to group features and then compute and present Shapley values for these groups instead of for all individual features. Reducing hundreds or thousands of features to half a dozen or so feature groups makes precise computations practically feasible, and the presentation and knowledge extraction greatly simplified. We give practical advice for using the approach and illustrate its usability in three different real world examples. The examples vary in both data type (regular tabular data and time series), feature dimension (medium to high), and application (insurance, genetics, and banking.}
  }


